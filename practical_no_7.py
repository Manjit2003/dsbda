# -*- coding: utf-8 -*-
"""Practical no. 7

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FN5REj_3PcA6lYKV9L1-Gsr5fCFHNg18
"""

import nltk
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")

# Tokenization
from nltk import word_tokenize, sent_tokenize

corpus = "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."
print(word_tokenize(corpus))
print(sent_tokenize(corpus))

# POS tagging
from nltk import pos_tag

tokens = word_tokenize(corpus)
print(pos_tag(tokens))

# Stop word removal
from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))
tokens = word_tokenize(corpus)
cleaned_tokens = []

for token in tokens:
    if (token not in stop_words):
        cleaned_tokens.append(token)
print(cleaned_tokens)

# Stemming
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
stemmed_tokens = []

for token in cleaned_tokens:
    stemmed = stemmer.stem(token)
    stemmed_tokens.append(stemmed)
print("Stemming\n",stemmed_tokens)

# Lemmatization
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = []

for token in cleaned_tokens:
    lemmatized = lemmatizer.lemmatize(token)
    lemmatized_tokens.append(lemmatized)
print(lemmatized_tokens)

# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = ["Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."]
vectorizer = TfidfVectorizer()
matrix = vectorizer.fit(corpus)
print(matrix.vocabulary_)

tfidf_matrix = vectorizer.transform(corpus)
print(tfidf_matrix)

print(vectorizer.get_feature_names_out())